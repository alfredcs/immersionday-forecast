{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Immersion Day\n",
    "\n",
    "This notebook will serve as a template for the overall process of taking a non ideal time series dataset and integrating it into [Amazon Forecast](https://aws.amazon.com/forecast/).\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Introduction to Amazon Forecast\n",
    "1. Obtaining Your Data\n",
    "1. Fitting the Data to Forecast\n",
    "1. Determining Your Forecast Horizon (1st pass)\n",
    "1. Building Your Predictors\n",
    "1. Visualizing Predictors\n",
    "1. Making Decisions\n",
    "1. Next Steps\n",
    "\n",
    "\n",
    "## Introduction to Amazon Forecast\n",
    "\n",
    "If you are not familiar with Amazon Forecast you can learn more about this tool on these pages:\n",
    "\n",
    "* [Product Page](https://aws.amazon.com/forecast/)\n",
    "* [GitHub Sample Notebooks](https://github.com/aws-samples/amazon-forecast-samples)\n",
    "* [Product Docs](https://docs.aws.amazon.com/forecast/latest/dg/what-is-forecast.html)\n",
    "\n",
    "\n",
    "## Obtaining Your Data\n",
    "\n",
    "A critical requirement to use Amazon Forecast is to have access to time-series data for your selected use case. To learn more about time series data:\n",
    "\n",
    "1. [Wikipedia](https://en.wikipedia.org/wiki/Time_series)\n",
    "1. [Toward's Data Science Primer](https://towardsdatascience.com/the-complete-guide-to-time-series-analysis-and-forecasting-70d476bfe775)\n",
    "1. [O'Reilly Book](https://www.amazon.com/gp/product/1492041653/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&psc=1)\n",
    "\n",
    "For this exercise, we use the individual household electric power consumption dataset. (Dua, D. and Karra Taniskidou, E. (2017). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.) We aggregate the usage data hourly.\n",
    "\n",
    "To begin, use Pandas to read the CSV and to show a sample of the data.\n",
    "\n",
    "To begin the cell below will complete the following:\n",
    "\n",
    "1. Create a directory for the data files.\n",
    "1. Download the sample data into the directory.\n",
    "1. Extract the archive file into the directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data downloaded, now we will import the Pandas library as well as a few other data science tools in order to inspect the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from time import sleep\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import pprint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib.dates as mdates\n",
    "import dateutil.parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>value</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-01 01:00:00</td>\n",
       "      <td>38.34991708126038</td>\n",
       "      <td>client_12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-01 02:00:00</td>\n",
       "      <td>33.5820895522388</td>\n",
       "      <td>client_12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-01-01 03:00:00</td>\n",
       "      <td>34.41127694859037</td>\n",
       "      <td>client_12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp              value       item\n",
       "0  2014-01-01 01:00:00  38.34991708126038  client_12\n",
       "1  2014-01-01 02:00:00   33.5820895522388  client_12\n",
       "2  2014-01-01 03:00:00  34.41127694859037  client_12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/item-demand-time.csv\", dtype = object, names=['timestamp','value','item'])\n",
    "df.drop(df.loc[df['item']!='client_12'].index, inplace=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>value</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7991</td>\n",
       "      <td>7991</td>\n",
       "      <td>7991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7991</td>\n",
       "      <td>2891</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2014-07-07 07:00:00</td>\n",
       "      <td>45.39800995024875</td>\n",
       "      <td>client_12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>7991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp              value       item\n",
       "count                  7991               7991       7991\n",
       "unique                 7991               2891          1\n",
       "top     2014-07-07 07:00:00  45.39800995024875  client_12\n",
       "freq                      1                 35       7991"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the output above there are 3 columns of data:\n",
    "\n",
    "1. The Timestamp\n",
    "1. A Value\n",
    "1. An Item\n",
    "\n",
    "These are the 3 key required pieces of information to generate a forecast with Amazon Forecast. More can be added but these 3 must always remain present.\n",
    "\n",
    "The dataset happens to span January 01, 2014 to Deceber 31, 2014. For our testing we would like to keep the last month of information in a different CSV. We are also going to save January to November to a different CSV as well.\n",
    "\n",
    "You may notice a variable named `df` this is a popular convention when using Pandas if you are using the library's dataframe object, it is similar to a table in a database. You can learn more here: https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html\n",
    "\n",
    "In our dataset we have information about 3 clients, lets focus on client_12 on this excercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_to_oct = df[(df['timestamp'] >= '2014-01-01') & (df['timestamp'] <= '2014-10-31')]\n",
    "remaining_df = df[(df['timestamp'] >= '2014-10-31') & (df['timestamp'] <= '2014-12-01')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now export them to CSV files and place them into your `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_to_oct.to_csv(\"../data/item-demand-time-train.csv\", header=False, index=False)\n",
    "remaining_df.to_csv(\"../data/item-demand-time-validation.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading your training data to S3\n",
    "\n",
    "At this time the data is ready to be sent to S3 where Forecast will use it later. The following cells will upload the data to S3.\n",
    "\n",
    "Please paste the Bucket Name and the Forecast Role ARN from your Cloudformation outputs section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this bucket name and your Role ARN \n",
    "\n",
    "bucket_name = \"sagemaker-studio-j1flqvl0t0p\"\n",
    "role_arn = \"arn:aws:iam::607620709047:role/forecast-s3-607620709047\"\n",
    "#role_arn = \"arn:aws:iam::607620709047:role/service-role/AmazonSageMaker-ExecutionRole-20210701T141013\"\n",
    "role_name = role_arn.split(\"/\")[1]\n",
    "\n",
    "target_time_series_filename =\"elec_data/item-demand-time-train.csv\"\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(target_time_series_filename).upload_file(\"../data/item-demand-time-train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started With Forecast\n",
    "\n",
    "Now that all of the required data to get started exists, our next step is to build the dataset groups and datasets required for our problem. Inside Amazon Forecast a DatasetGroup is an abstraction that contains all the datasets for a particular collection of Forecasts. There is no information sharing between DatasetGroups so if you'd like to try out various alternatives to the schemas we create below, you could create a new DatasetGroup and make your changes inside its corresponding Datasets.\n",
    "\n",
    "The order of the process below will be as follows:\n",
    "\n",
    "1. Create a DatasetGroup for our POC.\n",
    "1. Create a `Target-Time-Series` Dataset.\n",
    "1. Attach the Dataset to the DatasetGroup.\n",
    "1. Import the data into the Dataset.\n",
    "1. Generate Forecasts with ARIMA, Prophet, and DeepAR+.\n",
    "1. Query their Forecasts.\n",
    "1. Plot the Forecasts and metrics. \n",
    "\n",
    "\n",
    "At that point we can see which model is best and discuss how to add related data to our POC.\n",
    "\n",
    "The cell below defines a few global settings for our POC with the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FREQUENCY = \"H\" \n",
    "TIMESTAMP_FORMAT = \"yyyy-MM-dd hh:mm:ss\"\n",
    "\n",
    "project = 'forecast_immersion_day'\n",
    "datasetName= project+'_ds'\n",
    "datasetGroupName= project +'_dsg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the metada stored on this instance of a SageMaker Notebook determine the region we are operating in. If you are using a Jupyter Notebook outside of SageMaker simply define `region` as the string that indicates the region you would like to use for Forecast and S3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-west-2\n"
     ]
    }
   ],
   "source": [
    "with open('../data/resource-metadata.json') as notebook_info:\n",
    "    data = json.load(notebook_info)\n",
    "    resource_arn = data['ResourceArn']\n",
    "    region = resource_arn.split(':')[3]\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure your AWS APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(region_name=region) \n",
    "forecast = session.client(service_name='forecast') \n",
    "forecast_query = session.client(service_name='forecastquery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DatasetGroup\n",
    "\n",
    "create_dataset_group_response = forecast.create_dataset_group(DatasetGroupName=datasetGroupName,\n",
    "                                                              Domain=\"CUSTOM\",\n",
    "                                                             )\n",
    "datasetGroupArn = create_dataset_group_response['DatasetGroupArn']\n",
    "\n",
    "#Alfred datasetGroupArn = data['datasetGroupArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_dataset_group(DatasetGroupArn=datasetGroupArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the schema of your dataset here. Make sure the order of columns matches the raw data files.\n",
    "schema ={\n",
    "   \"Attributes\":[\n",
    "      {\n",
    "         \"AttributeName\":\"timestamp\",\n",
    "         \"AttributeType\":\"timestamp\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"target_value\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"item_id\",\n",
    "         \"AttributeType\":\"string\"\n",
    "      }\n",
    "   ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore next creation if it exists from previous job\n",
    "response=forecast.create_dataset(\n",
    "                    Domain=\"CUSTOM\",\n",
    "                    DatasetType='TARGET_TIME_SERIES',\n",
    "                    DatasetName=datasetName,\n",
    "                    DataFrequency=DATASET_FREQUENCY, \n",
    "                    Schema = schema\n",
    ")\n",
    "target_datasetArn = response['DatasetArn']\n",
    "\n",
    "#Alfred target_datasetArn = data['DatasetArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DatasetArn': 'arn:aws:forecast:us-west-2:607620709047:dataset/forecast_immersion_day_ds',\n",
       " 'DatasetName': 'forecast_immersion_day_ds',\n",
       " 'Domain': 'CUSTOM',\n",
       " 'DatasetType': 'TARGET_TIME_SERIES',\n",
       " 'DataFrequency': 'H',\n",
       " 'Schema': {'Attributes': [{'AttributeName': 'timestamp',\n",
       "    'AttributeType': 'timestamp'},\n",
       "   {'AttributeName': 'target_value', 'AttributeType': 'float'},\n",
       "   {'AttributeName': 'item_id', 'AttributeType': 'string'}]},\n",
       " 'EncryptionConfig': {},\n",
       " 'Status': 'ACTIVE',\n",
       " 'CreationTime': datetime.datetime(2021, 7, 1, 21, 58, 28, 95000, tzinfo=tzlocal()),\n",
       " 'LastModificationTime': datetime.datetime(2021, 7, 1, 21, 58, 28, 95000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': 'a35b0c97-f82c-48ee-8819-6437bfb1a19e',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Thu, 01 Jul 2021 22:56:58 GMT',\n",
       "   'x-amzn-requestid': 'a35b0c97-f82c-48ee-8819-6437bfb1a19e',\n",
       "   'content-length': '511',\n",
       "   'connection': 'keep-alive'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast.describe_dataset(DatasetArn=target_datasetArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '176cd880-b340-4b22-8ac4-6bbf00531585',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Thu, 01 Jul 2021 22:57:20 GMT',\n",
       "   'x-amzn-requestid': '176cd880-b340-4b22-8ac4-6bbf00531585',\n",
       "   'content-length': '2',\n",
       "   'connection': 'keep-alive'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attach the Dataset to the Dataset Group:\n",
    "forecast.update_dataset_group(DatasetGroupArn=datasetGroupArn, DatasetArns=[target_datasetArn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we can call import the dataset. Ignor ethe next lines if the DatasetImportJobArn has been created by previous jobs\n",
    "target_s3DataPath = \"s3://\"+bucket_name+\"/\"+target_time_series_filename\n",
    "datasetImportJobName = 'DSIMPORT_JOB_TARGET'\n",
    "\n",
    "ds_import_job_response=forecast.create_dataset_import_job(DatasetImportJobName=datasetImportJobName,\n",
    "                                                          DatasetArn=target_datasetArn,\n",
    "                                                          DataSource= {\n",
    "                                                              \"S3Config\" : {\n",
    "                                                                 \"Path\":target_s3DataPath,\n",
    "                                                                 #\"RoleArn\": data['ResourceArn'] #Get from the metadata file\n",
    "                                                                  \"RoleArn\": role_arn\n",
    "                                                              } \n",
    "                                                          },\n",
    "                                                          TimestampFormat=TIMESTAMP_FORMAT\n",
    "                                                         )\n",
    "ds_import_job_arn=ds_import_job_response['DatasetImportJobArn']\n",
    "\n",
    "while True:\n",
    "    dataImportStatus = forecast.describe_dataset_import_job(DatasetImportJobArn=ds_import_job_arn)['Status']\n",
    "    print(dataImportStatus)\n",
    "    if dataImportStatus != 'ACTIVE' and dataImportStatus != 'CREATE_FAILED':\n",
    "        sleep(30)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:forecast:us-west-2:607620709047:dataset-import-job/forecast_immersion_day_ds/DSIMPORT_JOB_TARGET\n"
     ]
    }
   ],
   "source": [
    "#s_import_job_arn=data['DatasetImportJobArn'] # Grab from the previous job. Comment it out if to create from scratch\n",
    "print(ds_import_job_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTIVE\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    dataImportStatus = forecast.describe_dataset_import_job(DatasetImportJobArn=ds_import_job_arn)['Status']\n",
    "    print(dataImportStatus)\n",
    "    if dataImportStatus != 'ACTIVE' and dataImportStatus != 'CREATE_FAILED':\n",
    "        sleep(30)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will run and poll every 30 seconds until the import process has completed. From there we will be able to create a few models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building bits\n",
    "\n",
    "Given that that our data is hourly and we want to generate a forecast on the hour, Forecast limits us to a horizon of 500 of whatever the slice is. This means we will be able to predict about 20 days into the future. In our case we are going to predict 3 days or 72 hours.\n",
    "\n",
    "The cells below will define a few variables to be used with all of our models. Then there will be an API call to create each `Predictor` where they are based on ARIMA, Prophet, and DeepAR+ respectfully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecastHorizon = 72\n",
    "NumberOfBacktestWindows = 1\n",
    "BackTestWindowOffset = 72\n",
    "ForecastFrequency = \"H\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built-in Forecast Algorithms\n",
    "Amazon Forecast provides six built-in algorithms for you to choose from. These range from commonly used statistical algorithms like Autoregressive Integrated Moving Average (ARIMA), to complex neural network algorithms like CNN-QR and DeepAR+.\n",
    "\n",
    "CNN-QR\n",
    "arn:aws:forecast:::algorithm/CNN-QR\n",
    "\n",
    "Amazon Forecast CNN-QR, Convolutional Neural Network - Quantile Regression, is a proprietary machine learning algorithm for forecasting time series using causal convolutional neural networks (CNNs). CNN-QR works best with large datasets containing hundreds of time series. It accepts item metadata, and is the only Forecast algorithm that accepts related time series data without future values.\n",
    "\n",
    "DeepAR+\n",
    "arn:aws:forecast:::algorithm/Deep_AR_Plus\n",
    "\n",
    "Amazon Forecast DeepAR+ is a proprietary machine learning algorithm for forecasting time series using recurrent neural networks (RNNs). DeepAR+ works best with large datasets containing hundreds of feature time series. The algorithm accepts forward-looking related time series and item metadata.\n",
    "\n",
    "Prophet\n",
    "arn:aws:forecast:::algorithm/Prophet\n",
    "\n",
    "Prophet is a time series forecasting algorithm based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality. It works best with time series with strong seasonal effects and several seasons of historical data.\n",
    "\n",
    "NPTS\n",
    "arn:aws:forecast:::algorithm/NPTS\n",
    "\n",
    "The Amazon Forecast Non-Parametric Time Series (NPTS) proprietary algorithm is a scalable, probabilistic baseline forecaster. NPTS is especially useful when working with sparse or intermittent time series. Forecast provides four algorithm variants: Standard NPTS, Seasonal NPTS, Climatological Forecaster, and Seasonal Climatological Forecaster.\n",
    "\n",
    "ARIMA\n",
    "arn:aws:forecast:::algorithm/ARIMA\n",
    "\n",
    "Autoregressive Integrated Moving Average (ARIMA) is a commonly used statistical algorithm for time-series forecasting. The algorithm is especially useful for simple datasets with under 100 time series.\n",
    "\n",
    "ETS\n",
    "arn:aws:forecast:::algorithm/ETS\n",
    "\n",
    "Exponential Smoothing (ETS) is a commonly used statistical algorithm for time-series forecasting. The algorithm is especially useful for simple datasets with under 100 time series, and datasets with seasonality patterns. ETS computes a weighted average over all observations in the time series dataset as its prediction, with exponentially decreasing weights over time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_algorithmArn = 'arn:aws:forecast:::algorithm/ARIMA'\n",
    "prophet_algorithmArn = 'arn:aws:forecast:::algorithm/Prophet'\n",
    "deepAR_Plus_algorithmArn = 'arn:aws:forecast:::algorithm/Deep_AR_Plus'\n",
    "cnnqr_algorithmArn = 'arn:aws:forecast:::algorithm/CNN-QR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA Specifics\n",
    "arima_predictorName= project+'_arima_algo_1'\n",
    "# Build ARIMA:\n",
    "arima_create_predictor_response=forecast.create_predictor(PredictorName=arima_predictorName, \n",
    "                                                  AlgorithmArn=arima_algorithmArn,\n",
    "                                                  ForecastHorizon=forecastHorizon,\n",
    "                                                  PerformAutoML= False,\n",
    "                                                  PerformHPO=False,\n",
    "                                                  EvaluationParameters= {\"NumberOfBacktestWindows\": NumberOfBacktestWindows, \n",
    "                                                                         \"BackTestWindowOffset\": BackTestWindowOffset}, \n",
    "                                                  InputDataConfig= {\"DatasetGroupArn\": datasetGroupArn},\n",
    "                                                  FeaturizationConfig= {\"ForecastFrequency\": ForecastFrequency, \n",
    "                                                                        \"Featurizations\": \n",
    "                                                                        [\n",
    "                                                                          {\"AttributeName\": \"target_value\", \n",
    "                                                                           \"FeaturizationPipeline\": \n",
    "                                                                            [\n",
    "                                                                              {\"FeaturizationMethodName\": \"filling\", \n",
    "                                                                               \"FeaturizationMethodParameters\": \n",
    "                                                                                {\"frontfill\": \"none\", \n",
    "                                                                                 \"middlefill\": \"zero\", \n",
    "                                                                                 \"backfill\": \"zero\"}\n",
    "                                                                              }\n",
    "                                                                            ]\n",
    "                                                                          }\n",
    "                                                                        ]\n",
    "                                                                       }\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet Specifics\n",
    "prophet_predictorName= project+'_prophet_algo_1'\n",
    "# Build Prophet:\n",
    "prophet_create_predictor_response=forecast.create_predictor(PredictorName=prophet_predictorName, \n",
    "                                                  AlgorithmArn=prophet_algorithmArn,\n",
    "                                                  ForecastHorizon=forecastHorizon,\n",
    "                                                  PerformAutoML= False,\n",
    "                                                  PerformHPO=False,\n",
    "                                                  EvaluationParameters= {\"NumberOfBacktestWindows\": NumberOfBacktestWindows, \n",
    "                                                                         \"BackTestWindowOffset\": BackTestWindowOffset}, \n",
    "                                                  InputDataConfig= {\"DatasetGroupArn\": datasetGroupArn},\n",
    "                                                  FeaturizationConfig= {\"ForecastFrequency\": ForecastFrequency, \n",
    "                                                                        \"Featurizations\": \n",
    "                                                                        [\n",
    "                                                                          {\"AttributeName\": \"target_value\", \n",
    "                                                                           \"FeaturizationPipeline\": \n",
    "                                                                            [\n",
    "                                                                              {\"FeaturizationMethodName\": \"filling\", \n",
    "                                                                               \"FeaturizationMethodParameters\": \n",
    "                                                                                {\"frontfill\": \"none\", \n",
    "                                                                                 \"middlefill\": \"zero\", \n",
    "                                                                                 \"backfill\": \"zero\"}\n",
    "                                                                              }\n",
    "                                                                            ]\n",
    "                                                                          }\n",
    "                                                                        ]\n",
    "                                                                       }\n",
    "                                                 )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepAR+ Specifics\n",
    "deepAR_Plus_predictorName= project+'_deeparp_algo_1'\n",
    "# Build DeepAR+:\n",
    "deeparp_create_predictor_response=forecast.create_predictor(PredictorName=deepAR_Plus_predictorName, \n",
    "                                                  AlgorithmArn=deepAR_Plus_algorithmArn,\n",
    "                                                  ForecastHorizon=forecastHorizon,\n",
    "                                                  PerformAutoML= False,\n",
    "                                                  PerformHPO=False,\n",
    "                                                  EvaluationParameters= {\"NumberOfBacktestWindows\": NumberOfBacktestWindows, \n",
    "                                                                         \"BackTestWindowOffset\": BackTestWindowOffset}, \n",
    "                                                  InputDataConfig= {\"DatasetGroupArn\": datasetGroupArn},\n",
    "                                                  FeaturizationConfig= {\"ForecastFrequency\": ForecastFrequency, \n",
    "                                                                        \"Featurizations\": \n",
    "                                                                        [\n",
    "                                                                          {\"AttributeName\": \"target_value\", \n",
    "                                                                           \"FeaturizationPipeline\": \n",
    "                                                                            [\n",
    "                                                                              {\"FeaturizationMethodName\": \"filling\", \n",
    "                                                                               \"FeaturizationMethodParameters\": \n",
    "                                                                                {\"frontfill\": \"none\", \n",
    "                                                                                 \"middlefill\": \"zero\", \n",
    "                                                                                 \"backfill\": \"zero\"}\n",
    "                                                                              }\n",
    "                                                                            ]\n",
    "                                                                          }\n",
    "                                                                        ]\n",
    "                                                                       }\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNNQP Specifics\n",
    "cnnqr_predictorName= project+'_cnnqp_algo_1'\n",
    "# Build DeepAR+:\n",
    "cnnqr_create_predictor_response=forecast.create_predictor(PredictorName=cnnqr_predictorName, \n",
    "                                                  AlgorithmArn=cnnqr_algorithmArn,\n",
    "                                                  ForecastHorizon=forecastHorizon,\n",
    "                                                  PerformAutoML= False,\n",
    "                                                  PerformHPO=False,\n",
    "                                                  EvaluationParameters= {\"NumberOfBacktestWindows\": NumberOfBacktestWindows, \n",
    "                                                                         \"BackTestWindowOffset\": BackTestWindowOffset}, \n",
    "                                                  InputDataConfig= {\"DatasetGroupArn\": datasetGroupArn},\n",
    "                                                  FeaturizationConfig= {\"ForecastFrequency\": ForecastFrequency, \n",
    "                                                                        \"Featurizations\": \n",
    "                                                                        [\n",
    "                                                                          {\"AttributeName\": \"target_value\", \n",
    "                                                                           \"FeaturizationPipeline\": \n",
    "                                                                            [\n",
    "                                                                              {\"FeaturizationMethodName\": \"filling\", \n",
    "                                                                               \"FeaturizationMethodParameters\": \n",
    "                                                                                {\"frontfill\": \"none\", \n",
    "                                                                                 \"middlefill\": \"zero\", \n",
    "                                                                                 \"backfill\": \"zero\"}\n",
    "                                                                              }\n",
    "                                                                            ]\n",
    "                                                                          }\n",
    "                                                                        ]\n",
    "                                                                       }\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These calls will take an hour or so to complete in full. So feel free to take lunch here, go grab a pint, really anything that is going to kill a decent volume of time.\n",
    "\n",
    "The following while loop keeps track of the DeepAR+ predictor progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "while True:\n",
    "    status = forecast.describe_predictor(PredictorArn=deeparp_create_predictor_response['PredictorArn'])['Status']\n",
    "    print(status)\n",
    "    if status != 'ACTIVE' and status != 'CREATE_FAILED':\n",
    "        sleep(30)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the Models\n",
    "\n",
    "First we are going to get the metrics for each model and see how they stack up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA Metrics\n",
    "#arima_arn = arima_create_predictor_response['PredictorArn']\n",
    "arima_arn = data['arimaPredictorArn'] # Take from trained predictor from a previous job\n",
    "arima_metrics = forecast.get_accuracy_metrics(PredictorArn=arima_arn)\n",
    "pp = pprint.PrettyPrinter()\n",
    "pp.pprint(arima_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet Metrics\n",
    "#prophet_arn = prophet_create_predictor_response['PredictorArn']\n",
    "prophet_arn = data['prophetPredictorArn'] # Take from trained predictor from a previous job\n",
    "prophet_metrics = forecast.get_accuracy_metrics(PredictorArn=prophet_arn)\n",
    "pp = pprint.PrettyPrinter()\n",
    "pp.pprint(prophet_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following 2 methods are based on deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepAR+ Metrics\n",
    "#deeparp_arn = deeparp_create_predictor_response['PredictorArn']\n",
    "deeparp_arn = data['deeparpPredictorArn'] # Take from trained predictor from a previous job\n",
    "deeparp_metrics = forecast.get_accuracy_metrics(PredictorArn=deeparp_arn)\n",
    "pp = pprint.PrettyPrinter()\n",
    "pp.pprint(deeparp_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNNQR Metrics\n",
    "#cnnqr_arn = cnnqr_create_predictor_response['PredictorArn']\n",
    "cnnqr_arn = data['cnnqrPredictorArn'] # Take from trained predictor from a previous job\n",
    "cnnqr_metrics = forecast.get_accuracy_metrics(PredictorArn=cnnqr_arn)\n",
    "pp = pprint.PrettyPrinter()\n",
    "pp.pprint(cnnqr_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Forecast For Each\n",
    "\n",
    "The next phase is to generate a Forecast from each Predictor so we can see the results and understand visually which model is performing better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA\n",
    "arima_forecastName = project+'_arima_algo_forecast'\n",
    "arima_create_forecast_response=forecast.create_forecast(ForecastName=arima_forecastName,\n",
    "                                                  PredictorArn=arima_arn)\n",
    "arima_forecast_arn = arima_create_forecast_response['ForecastArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet\n",
    "prophet_forecastName = project+'_prophet_algo_forecast'\n",
    "prophet_create_forecast_response=forecast.create_forecast(ForecastName=prophet_forecastName,\n",
    "                                                  PredictorArn=prophet_arn)\n",
    "prophet_forecast_arn = prophet_create_forecast_response['ForecastArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepAR+\n",
    "deeparp_forecastName = project+'_deeparp_algo_forecast'\n",
    "deeparp_create_forecast_response=forecast.create_forecast(ForecastName=deeparp_forecastName,\n",
    "                                                  PredictorArn=deeparp_arn)\n",
    "deeparp_forecast_arn = deeparp_create_forecast_response['ForecastArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN-QR\n",
    "cnnqr_forecastName = project+'_cnnqr_algo_forecast'\n",
    "cnnqr_create_forecast_response=forecast.create_forecast(ForecastName=cnnqr_forecastName,\n",
    "                                                  PredictorArn=cnnqr_arn)\n",
    "cnnqr_forecast_arn = cnnqr_create_forecast_response['ForecastArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "while True:\n",
    "    status = forecast.describe_forecast(ForecastArn=cnnqr_forecast_arn)['Status']\n",
    "    print(status)\n",
    "    if status != 'ACTIVE' and status != 'CREATE_FAILED':\n",
    "        sleep(30)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting your Forecasts to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepAR + Forecast\n",
    "\n",
    "deep_ar_path = \"s3://\" + bucket_name + \"/DeepAR\"\n",
    "deep_ar_job_name = \"mlimday_deep_ar_algo_forecast\"\n",
    "forecast.create_forecast_export_job(ForecastExportJobName=deep_ar_job_name,\n",
    "                                    ForecastArn=deeparp_forecast_arn,\n",
    "                                    Destination={\n",
    "                                        \"S3Config\": {\n",
    "                                            \"Path\": deep_ar_path,\n",
    "                                            \"RoleArn\": role_arn\n",
    "                                        }\n",
    "                                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arima Forecast\n",
    "\n",
    "arima_path = \"s3://\" + bucket_name + \"/Arima\"\n",
    "arima_job_name = \"mlimday_arima_algo_forecast\"\n",
    "forecast.create_forecast_export_job(ForecastExportJobName=arima_job_name,\n",
    "                                    ForecastArn=arima_forecast_arn,\n",
    "                                    Destination={\n",
    "                                        \"S3Config\": {\n",
    "                                            \"Path\": arima_path,\n",
    "                                            \"RoleArn\": role_arn\n",
    "                                        }\n",
    "                                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet Forecast\n",
    "\n",
    "prophet_path = \"s3://\" + bucket_name + \"/Prophet\"\n",
    "prophet_job_name = \"mlimday_prophet_algo_forecast\"\n",
    "forecast.create_forecast_export_job(ForecastExportJobName=prophet_job_name,\n",
    "                                    ForecastArn=prophet_forecast_arn,\n",
    "                                    Destination={\n",
    "                                        \"S3Config\": {\n",
    "                                            \"Path\": prophet_path,\n",
    "                                            \"RoleArn\": role_arn\n",
    "                                        }\n",
    "                                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN-QR Forecast\n",
    "\n",
    "cnnqr_path = \"s3://\" + bucket_name + \"/CNNQR\"\n",
    "cnnqr_job_name = \"mlimday_prophet_algo_forecast\"\n",
    "forecast.create_forecast_export_job(ForecastExportJobName=cnnqr_job_name,\n",
    "                                    ForecastArn=cnnqr_forecast_arn,\n",
    "                                    Destination={\n",
    "                                        \"S3Config\": {\n",
    "                                            \"Path\": cnnqr_path,\n",
    "                                            \"RoleArn\": role_arn\n",
    "                                        }\n",
    "                                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exporting process is another one of those items that will take **5 minutes** to complete. Just poll for progress in the console. From the earlier page where you saw the status turn `Active` for a Forecast, click it and you can see the progress of the export."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the Forecasts\n",
    "\n",
    "At this point they are all exported into S3 but you need to obtain the results locally so we can explore them, the cells below will do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlimday_deep_ar_algo_forecast_2021-05-24T23-43-45Z_part0.csv\n",
      "mlimday_arima_algo_forecast_2021-05-24T23-30-54Z_part0.csv\n",
      "mlimday_prophet_algo_forecast_2021-05-24T23-47-49Z_part0.csv\n",
      "mlimday_cnnqr_algo_forecast_2021-05-25T01-28-50Z_part0.csv\n"
     ]
    }
   ],
   "source": [
    "# DeepAR File\n",
    "s3 = boto3.resource('s3')\n",
    "s3_bucket = s3.Bucket(bucket_name)\n",
    "deep_ar_filename = \"\"\n",
    "deep_ar_files = list(s3_bucket.objects.filter(Prefix=\"DeepAR\"))\n",
    "for file in deep_ar_files:\n",
    "    #There will be a collection of CSVs if the forecast is large, modify this to go get them all\n",
    "    if \"csv\" in file.key and 'deep_ar' in file.key:\n",
    "        deep_ar_filename = file.key.split('/')[1]\n",
    "        s3.Bucket(bucket_name).download_file(file.key, \"../data/\"+deep_ar_filename)\n",
    "print(deep_ar_filename)\n",
    "\n",
    "# ARIMA File\n",
    "arima_filename = \"\"\n",
    "arima_files = list(s3_bucket.objects.filter(Prefix=\"Arima\"))\n",
    "for file in arima_files:\n",
    "    #There will be a collection of CSVs if the forecast is large, modify this to go get them all\n",
    "    if \"csv\" in file.key and 'arima' in file.key:\n",
    "        arima_filename = file.key.split('/')[1]\n",
    "        s3.Bucket(bucket_name).download_file(file.key, \"../data/\"+arima_filename)\n",
    "print(arima_filename)\n",
    "\n",
    "# Phrophet File\n",
    "prophet_filename = \"\"\n",
    "prophet_files = list(s3_bucket.objects.filter(Prefix=\"Prophet\"))\n",
    "for file in prophet_files:\n",
    "    #There will be a collection of CSVs if the forecast is large, modify this to go get them all\n",
    "    if \"csv\" in file.key and 'prophet' in file.key:\n",
    "        prophet_filename = file.key.split('/')[1]\n",
    "        s3.Bucket(bucket_name).download_file(file.key, \"../data/\"+prophet_filename)\n",
    "print(prophet_filename)\n",
    "\n",
    "# CNNQR File\n",
    "cnnqr_filename = \"\"\n",
    "cnnqr_files = list(s3_bucket.objects.filter(Prefix=\"CNNQR\"))\n",
    "for file in cnnqr_files:\n",
    "    #There will be a collection of CSVs if the forecast is large, modify this to go get them all\n",
    "    if \"csv\" in file.key and 'cnnqr' in file.key:\n",
    "        cnnqr_filename = file.key.split('/')[1]\n",
    "        s3.Bucket(bucket_name).download_file(file.key, \"../data/\"+cnnqr_filename)\n",
    "print(cnnqr_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Forecast\n",
    "\n",
    "Event before exporting the forecasts themselves we can see a few things in the logs above...\n",
    "\n",
    "Mainly, the RMSE for each model:\n",
    "\n",
    "1. ARIMA - RMSE: 20.900040194709177,\n",
    "1. Prophet - RMSE: 20.298012475873875\n",
    "1. DeepAR+ - RMSE: 8.336288046896607\n",
    "\n",
    "Those numbers are:\n",
    "\n",
    "1. ARIMA wQL[0.5]: 0.15327057905199545\n",
    "1. Prophet wQL[0.5]: 0.16823164160633303\n",
    "1. DeepAR+ wQL[0.5]: 0.05955031556311987\n",
    "\n",
    "\n",
    "This tells us that our DeepAR+ model is doing the best when evaluating the p50 result.\n",
    "\n",
    "The next stage would be to plot these numbers over a particular window.\n",
    "\n",
    "To make this particular process easier we are going to export them all as CSV's from the console then read them in later. An improvement would be to use the JSON API and convert to a DF that way.\n",
    "\n",
    "Note the files were downloaded and placed into the `../data/` folder for exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>date</th>\n",
       "      <th>p10</th>\n",
       "      <th>p50</th>\n",
       "      <th>p90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>client_12</td>\n",
       "      <td>2014-10-31T22:00:00Z</td>\n",
       "      <td>23.077428</td>\n",
       "      <td>66.626767</td>\n",
       "      <td>110.176107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      item_id                  date        p10        p50         p90\n",
       "22  client_12  2014-10-31T22:00:00Z  23.077428  66.626767  110.176107"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ARIMA Eval\n",
    "arima_predicts = pd.read_csv(\"../data/\" + arima_filename)\n",
    "arima_predicts.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_predicts.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the timezone\n",
    "arima_predicts['date'] = pd.to_datetime(arima_predicts['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>date</th>\n",
       "      <th>p10</th>\n",
       "      <th>p50</th>\n",
       "      <th>p90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>client_12</td>\n",
       "      <td>2014-11-01 02:00:00+00:00</td>\n",
       "      <td>6.988902</td>\n",
       "      <td>50.930664</td>\n",
       "      <td>94.872427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      item_id                      date       p10        p50        p90\n",
       "26  client_12 2014-11-01 02:00:00+00:00  6.988902  50.930664  94.872427"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arima_predicts.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_predicts['date'] = arima_predicts['date'].dt.tz_convert(None)\n",
    "arima_predicts.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_predicts.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-10-31 00:00:00\n",
      "2014-11-02 23:00:00\n"
     ]
    }
   ],
   "source": [
    "print (arima_predicts.index.min())\n",
    "print (arima_predicts.index.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see our prediction goes from October 31st to November 2nd as expectged given our 72 hour interval forecast horizon. Also we can see the cyclical nature of the predictions over the entire timeframe. \n",
    "\n",
    "Now we are going to create a dataframe of the prediction values from this Forecast and the actual values.\n",
    "\n",
    "First let us remove the column ID of item before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_predicts = arima_predicts[['p10', 'p50', 'p90']]\n",
    "arima_predicts.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 72 entries, 2014-10-31 00:00:00 to 2014-11-02 23:00:00\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   p10     72 non-null     float64\n",
      " 1   p50     72 non-null     float64\n",
      " 2   p90     72 non-null     float64\n",
      "dtypes: float64(3)\n",
      "memory usage: 2.2 KB\n"
     ]
    }
   ],
   "source": [
    "# Now strip the timezone information\n",
    "arima_predicts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>value</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>2014-11-29 19:00:00</td>\n",
       "      <td>47.263682</td>\n",
       "      <td>client_12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>2014-11-29 20:00:00</td>\n",
       "      <td>51.616915</td>\n",
       "      <td>client_12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>2014-11-29 21:00:00</td>\n",
       "      <td>49.129353</td>\n",
       "      <td>client_12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>2014-11-29 22:00:00</td>\n",
       "      <td>41.666667</td>\n",
       "      <td>client_12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>2014-11-29 23:00:00</td>\n",
       "      <td>42.703151</td>\n",
       "      <td>client_12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               timestamp      value       item\n",
       "715  2014-11-29 19:00:00  47.263682  client_12\n",
       "716  2014-11-29 20:00:00  51.616915  client_12\n",
       "717  2014-11-29 21:00:00  49.129353  client_12\n",
       "718  2014-11-29 22:00:00  41.666667  client_12\n",
       "719  2014-11-29 23:00:00  42.703151  client_12"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_df = pd.read_csv(\"../data/item-demand-time-validation.csv\", names=['timestamp','value','item'])\n",
    "actual_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_df = actual_df[(actual_df['timestamp'] >= '2014-10-31') & (actual_df['timestamp'] < '2014-11-03')]\n",
    "\n",
    "results_df = pd.DataFrame(columns=['timestamp', 'value', 'source'])\n",
    "for index, row in actual_df.iterrows():\n",
    "    clean_timestamp = dateutil.parser.parse(row['timestamp'])\n",
    "    results_df = results_df.append({'timestamp' : clean_timestamp , 'value' : row['value'], 'source': 'actual'} , ignore_index=True)\n",
    "                                   \n",
    "validation_df = results_df.pivot(columns='source', values='value', index=\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally let us join the dataframes together\n",
    "arima_val_df = arima_predicts.join(validation_df, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "arima_val_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prophet Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet Eval\n",
    "prophet_predicts = pd.read_csv(\"../data/\"+prophet_filename)\n",
    "prophet_predicts.sample()\n",
    "# Remove the timezone\n",
    "prophet_predicts['date'] = pd.to_datetime(prophet_predicts['date'])\n",
    "prophet_predicts['date'] = prophet_predicts['date'].dt.tz_convert(None)\n",
    "prophet_predicts.set_index('date', inplace=True)\n",
    "prophet_predicts = prophet_predicts[['p10', 'p50', 'p90']]\n",
    "# Finally let us join the dataframes together\n",
    "prophet_val_df = prophet_predicts.join(validation_df, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "prophet_val_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepAR+ Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepAR+ Eval\n",
    "deeparp_predicts = pd.read_csv(\"../data/\"+deep_ar_filename)\n",
    "deeparp_predicts.sample()\n",
    "# Remove the timezone\n",
    "deeparp_predicts['date'] = pd.to_datetime(deeparp_predicts['date'])\n",
    "deeparp_predicts['date'] = deeparp_predicts['date'].dt.tz_convert(None)\n",
    "deeparp_predicts.set_index('date', inplace=True)\n",
    "deeparp_predicts = deeparp_predicts[['p10', 'p50', 'p90']]\n",
    "# Finally let us join the dataframes together\n",
    "deeparp_val_df = deeparp_predicts.join(validation_df, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "deeparp_val_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is particularly interesting here is that we were below the actual numbers for a good portion of the day even with p90. We did see great performance from Prophet and the metrics indicate that DeepAR+ is objectively better here so now we will add related time series data to our project and see how the models behave then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNQR Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNNQR+ Eval\n",
    "cnnqr_predicts = pd.read_csv(\"../data/\"+cnnqr_filename)\n",
    "cnnqr_predicts.sample()\n",
    "# Remove the timezone\n",
    "cnnqr_predicts['date'] = pd.to_datetime(cnnqr_predicts['date'])\n",
    "cnnqr_predicts['date'] = cnnqr_predicts['date'].dt.tz_convert(None)\n",
    "cnnqr_predicts.set_index('date', inplace=True)\n",
    "cnnqr_predicts = cnnqr_predicts[['p10', 'p50', 'p90']]\n",
    "# Finally let us join the dataframes together\n",
    "cnnqr_val_df = cnnqr_predicts.join(validation_df, how='outer')\n",
    "\n",
    "# Plot\n",
    "cnnqr_val_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting it all together\n",
    "\n",
    "Lastly we will take a look at our p50 prediction from ARIMA, Prophet, and DeepAR+ where the latter leverage related time series data to see how close they are over our validation period.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>ARIMA</th>\n",
       "      <th>PROPHET</th>\n",
       "      <th>DEEPARP</th>\n",
       "      <th>CNNQR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-10-31 00:00:00</th>\n",
       "      <td>62.396352</td>\n",
       "      <td>49.739619</td>\n",
       "      <td>72.251999</td>\n",
       "      <td>60.178146</td>\n",
       "      <td>48.814850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-31 01:00:00</th>\n",
       "      <td>59.286899</td>\n",
       "      <td>50.541509</td>\n",
       "      <td>69.781819</td>\n",
       "      <td>59.220978</td>\n",
       "      <td>49.882278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-31 02:00:00</th>\n",
       "      <td>49.129353</td>\n",
       "      <td>52.032872</td>\n",
       "      <td>70.902317</td>\n",
       "      <td>58.893467</td>\n",
       "      <td>53.030327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-31 03:00:00</th>\n",
       "      <td>55.555556</td>\n",
       "      <td>50.317142</td>\n",
       "      <td>71.824259</td>\n",
       "      <td>52.738972</td>\n",
       "      <td>38.027752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-31 04:00:00</th>\n",
       "      <td>50.580431</td>\n",
       "      <td>49.509088</td>\n",
       "      <td>75.248299</td>\n",
       "      <td>52.232540</td>\n",
       "      <td>39.604553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        actual      ARIMA    PROPHET    DEEPARP      CNNQR\n",
       "timestamp                                                                 \n",
       "2014-10-31 00:00:00  62.396352  49.739619  72.251999  60.178146  48.814850\n",
       "2014-10-31 01:00:00  59.286899  50.541509  69.781819  59.220978  49.882278\n",
       "2014-10-31 02:00:00  49.129353  52.032872  70.902317  58.893467  53.030327\n",
       "2014-10-31 03:00:00  55.555556  50.317142  71.824259  52.738972  38.027752\n",
       "2014-10-31 04:00:00  50.580431  49.509088  75.248299  52.232540  39.604553"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a final df\n",
    "validation_df_final = validation_df.copy()\n",
    "validation_df_final = validation_df_final[['actual']]\n",
    "# ARIMA P50\n",
    "arima_p50 = arima_predicts.copy()\n",
    "arima_p50 = arima_p50[['p50']]\n",
    "arima_p50.rename(columns = {'p50':'ARIMA'}, inplace = True)\n",
    "# Prophet P50:\n",
    "prophet_val_p50 = prophet_predicts.copy()\n",
    "prophet_val_p50 = prophet_val_p50[['p50']]\n",
    "prophet_val_p50.rename(columns = {'p50':'PROPHET'}, inplace = True)\n",
    "# DeepAR+ P50:\n",
    "deeparp_val_p50 = deeparp_predicts.copy()\n",
    "deeparp_val_p50 = deeparp_val_p50[['p50']]\n",
    "deeparp_val_p50.rename(columns = {'p50':'DEEPARP'}, inplace = True)\n",
    "# CNNQR+ P50:\n",
    "cnnqr_val_p50 = cnnqr_predicts.copy()\n",
    "cnnqr_val_p50 = cnnqr_val_p50[['p50']]\n",
    "cnnqr_val_p50.rename(columns = {'p50':'CNNQR'}, inplace = True)\n",
    "\n",
    "# Join DFs\n",
    "validation_df_final = validation_df_final.join(arima_p50, how='outer')\n",
    "validation_df_final = validation_df_final.join(prophet_val_p50, how='outer')\n",
    "validation_df_final = validation_df_final.join(deeparp_val_p50, how='outer')\n",
    "validation_df_final = validation_df_final.join(cnnqr_val_p50, how='outer')\n",
    "validation_df_final.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df_final.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(\"DeepAR/mlimday_deep_ar_algo_forecast_2020-04-16T18-12-03Z_part0.csv\").delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store datasetGroupArn\n",
    "%store target_datasetArn\n",
    "%store role_name\n",
    "%store key\n",
    "%store bucket_name\n",
    "%store region\n",
    "%store ds_import_job_arn\n",
    "%store prophet_forecast_arn\n",
    "%store arima_forecast_arn\n",
    "%store deeparp_forecast_arn\n",
    "%store arima_arn\n",
    "%store prophet_arn\n",
    "%store deeparp_arn\n",
    "%store deep_ar_filename\n",
    "%store arima_filename\n",
    "%store prophet_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (SageMaker JumpStart PyTorch 1.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:787968894560:image/sagemaker-jumpstart-pytorch-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
